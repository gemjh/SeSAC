import os
import glob
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from itertools import product
import time
import warnings
warnings.filterwarnings('ignore')

# ptk_sound.pyì˜ í•¨ìˆ˜ë“¤ì„ ì„í¬íŠ¸
import sys
sys.path.append('/Volumes/SSAM/aptk_model')

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
import librosa
import scipy.signal
from pydub import AudioSegment

def convert_audio_for_model(user_file, output_file=None, EXPECTED_SAMPLE_RATE=16000):
    """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ëª¨ë¸ì— ë§ê²Œ ë³€í™˜ (ëª¨ë…¸ì±„ë„ + ì •ê·œí™”)"""
    import tempfile
    import uuid
    
    # ì„ì‹œ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì‚­ì œë˜ë„ë¡ NamedTemporaryFile ì‚¬ìš©
    temp_file = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
    temp_file.close()
    
    try:
        audio = AudioSegment.from_file(user_file)
        audio = audio.set_frame_rate(EXPECTED_SAMPLE_RATE).set_channels(1)
        audio.export(temp_file.name, format="wav")
        return temp_file.name
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(temp_file.name):
            os.unlink(temp_file.name)
        raise e

# def count_peaks_from_waveform_optimized(filepath, n=3, height=0.36, distance=2500, plot=False):
#     """
#     ìµœì í™”ëœ í”¼í¬ ì¹´ìš´íŒ… í•¨ìˆ˜ (ì„ì‹œ íŒŒì¼ ì¦‰ì‹œ ì •ë¦¬ í¬í•¨)
#     """
#     converted_file = None
#     try:
#         # í•œ ë²ˆë§Œ ë³€í™˜í•˜ê³  ì¦‰ì‹œ ì‚¬ìš©
#         converted_file = convert_audio_for_model(filepath)
#         y, sr = librosa.load(converted_file, sr=None)
        
#         # ì„ì‹œ íŒŒì¼ì„ ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ì— ì´ë¯¸ ë¡œë“œë¨)
#         if converted_file and os.path.exists(converted_file):
#             try:
#                 os.unlink(converted_file)
#                 converted_file = None  # ì‚­ì œë˜ì—ˆìŒì„ í‘œì‹œ
#             except:
#                 pass
        
#         # nì´ˆ ìë¥´ê¸°
#         y_trimmed = y[:sr * int(n)]
        
#         if np.max(np.abs(y_trimmed)) == 0:
#             return 0
            
#         y_trimmed = y_trimmed / np.max(np.abs(y_trimmed))
        
#         # í”¼í¬ íƒì§€
#         peaks, _ = scipy.signal.find_peaks(y_trimmed, height=height, distance=distance)
        
#         if plot:
#             plt.figure(figsize=(12, 4))
#             plt.plot(y_trimmed)
#             plt.plot(peaks, y_trimmed[peaks], "rx")
#             plt.title(f"Detected Peaks: {len(peaks)}")
#             plt.show()
            
#         # peakê°€ 2ê°œ ì´ìƒì¼ ë•Œë§Œ ì‹œê°„ ê¸°ë°˜ ê³„ì‚° ê°€ëŠ¥
#         if len(peaks) >= 2:
#             # ì²« ë²ˆì§¸ peakì™€ ë§ˆì§€ë§‰ peak ì‚¬ì´ì˜ ì‹œê°„ ê³„ì‚° (ì´ˆ ë‹¨ìœ„)
#             first_peak_time = peaks[0] / sr
#             last_peak_time = peaks[-1] / sr
#             time_length = last_peak_time - first_peak_time
            
#             # ì‹œê°„ì´ 0ë³´ë‹¤ í´ ë•Œë§Œ ë‚˜ëˆ„ê¸°
#             if time_length > 0:
#                 return len(peaks) / time_length  # í‰ê·  peak rate (peaks per second)
#             else:
#                 return len(peaks)  # ì‹œê°„ ì°¨ì´ê°€ 0ì´ë©´ ë‹¨ìˆœ ê°œìˆ˜ ë°˜í™˜
#         elif len(peaks) == 1:
#             return 0  # peakê°€ 1ê°œë©´ rate ê³„ì‚° ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ 0
#         else:
#             return None  # peakê°€ ì—†ìœ¼ë©´ null
            
#     except Exception as e:
#         print(f"âš ï¸ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {filepath}: {e}")
#         return 0
#     finally:
#         # í˜¹ì‹œ ë‚¨ì•„ìˆëŠ” ì„ì‹œ íŒŒì¼ ì •ë¦¬
#         if converted_file and os.path.exists(converted_file):
#             try:
#                 os.unlink(converted_file)
#             except:
#                 pass

def count_peaks_from_waveform_wholetime(filepath, n=10, height=0.14, distance=2400, plot=False):
    """
    íŒŒì¼ ì „ì²´ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ peak rateë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    """
    converted_file = None
    try:
        # í•œ ë²ˆë§Œ ë³€í™˜í•˜ê³  ì¦‰ì‹œ ì‚¬ìš©
        converted_file = convert_audio_for_model(filepath)
        y, sr = librosa.load(converted_file, sr=None)
        
        # ì„ì‹œ íŒŒì¼ì„ ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ì— ì´ë¯¸ ë¡œë“œë¨)
        if converted_file and os.path.exists(converted_file):
            try:
                os.unlink(converted_file)
                converted_file = None  # ì‚­ì œë˜ì—ˆìŒì„ í‘œì‹œ
            except:
                pass

        # nì´ˆ ìë¥´ê¸°
        y_trimmed = y[:sr * int(n)]
        
        if np.max(np.abs(y_trimmed)) == 0:
            return 0
            
        y_trimmed = y_trimmed / np.max(np.abs(y_trimmed))
        
        # í”¼í¬ íƒì§€
        peaks, _ = scipy.signal.find_peaks(y_trimmed, height=height, distance=distance)
        
        if plot:
            plt.figure(figsize=(12, 4))
            plt.plot(y_trimmed)
            plt.plot(peaks, y_trimmed[peaks], "rx")
            plt.title(f"Detected Peaks: {len(peaks)}")
            plt.show()
        
        # y_trimmedì˜ ì´ ì‹œê°„ ê³„ì‚°
        y_trimmed_total_time = len(y_trimmed) / sr
        
        if len(peaks) > 0 and y_trimmed_total_time > 0:
            return len(peaks) / y_trimmed_total_time  # y_trimmed ì‹œê°„ ëŒ€ë¹„ peak rate (ì´ˆë‹¹ peak ê°œìˆ˜)
        elif len(peaks) == 0:
            return None  # peakê°€ ì—†ìœ¼ë©´ null
        else:
            return 0  # ì‹œê°„ì´ 0ì´ë©´ 0
            
    except Exception as e:
        print(f"âš ï¸ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {filepath}: {e}")
        return 0
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if converted_file and os.path.exists(converted_file):
            try:
                os.unlink(converted_file)
            except:
                pass

def optimize_ptk_parameters():
    """
    PTK íŒŒë¼ë¯¸í„° ìµœì í™” íŒŒì´í”„ë¼ì¸
    """
    print("ğŸ” PTK íŒŒë¼ë¯¸í„° ìµœì í™” íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
    
    # ìµœì í™”í•  íŒŒë¼ë¯¸í„° ë²”ìœ„ ì„¤ì • (ì›ë³¸ íŒŒë¼ë¯¸í„° ì£¼ë³€ì—ì„œ ì„¸ë°€í•˜ê²Œ)
    # height_values = np.arange(0.1, 0.5, 0.01)  # 0.02 ~ 0.115, 0.005 ê°„ê²© (ì›ë³¸ 0.05 ì£¼ë³€)
    height_values=np.arange(.1,.25,.01)
    distance_values = np.arange(2400, 3000, 100)  # 2000 ~ 4800, 200 ê°„ê²© (ì›ë³¸ 3000 ì£¼ë³€)
    n_values=np.arange(8,12,1)
    
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸í•  ì¡°í•© ìˆ˜: {len(height_values) * len(distance_values)*len(n_values)}")
    
    # ë°ì´í„° ë¡œë“œ
    folder = '/Volumes/SSAM/project/files/upload'
    pattern = os.path.join(folder, '**', 'CLAP_D', '1', 'p*.wav')
    wav_files = glob.glob(pattern, recursive=True)
    print(f"ğŸµ ë°œê²¬ëœ WAV íŒŒì¼ ìˆ˜: {len(wav_files)}")
    
    # ë¼ë²¨ ë°ì´í„° ë¡œë“œ
    df = pd.read_csv('/Volumes/SSAM/aptk_model/labeled_data_D.csv', header=1, index_col=0)
    ptk_label = df.loc[:, ['peo_3rd', 'keo_3rd']]
    
    # ìµœì í™” ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    optimization_results = []
    
    total_combinations = len(height_values) * len(distance_values)*len(n_values)
    combination_count = 0
    
    print("ğŸš€ ìµœì í™” ì‹œì‘...")
    
    for height in height_values:
     for n in n_values:
        for distance in distance_values:
            combination_count += 1
            
            print(f"\nğŸ“ˆ ì§„í–‰ë¥ : {combination_count}/{total_combinations} "
                  f"({combination_count/total_combinations*100:.1f}%)")
            print(f"ğŸ¯ í˜„ì¬ í…ŒìŠ¤íŠ¸: height={height:.3f}, distance={int(distance)},n={n}")
            
            try:
                start_time = time.time()
                
                # í˜„ì¬ íŒŒë¼ë¯¸í„°ë¡œ ê²°ê³¼ ê³„ì‚°
                ptk_result = pd.DataFrame(columns=['peo_3rd', 'keo_3rd'])
                
                for wav_file in wav_files:
                    # Windows ê²½ë¡œ ì²˜ë¦¬ë¥¼ ìœ„í•´ replaceë¥¼ ìˆ˜ì •
                    parts = wav_file.replace(folder, '').strip(os.sep).split(os.sep)
                    if len(parts) > 0:
                        index_key = parts[0]
                    else:
                        index_key = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(wav_file))))
                    
                    if index_key not in ptk_result.index:
                        ptk_result.loc[index_key] = [None, None]
                    
                    filename = os.path.basename(wav_file)
                    
                    # ê° ìŒì„± ìœ í˜•ë³„ë¡œ í”¼í¬ ì¹´ìš´íŠ¸
                    if 'p_3_' in filename:  # peo_3rd
                        result = count_peaks_from_waveform_wholetime(wav_file, n=n,height=height, distance=distance)
                        ptk_result.loc[index_key, 'peo_3rd'] = result
                    # elif 'p_6_' in filename:  # teo_3rd
                    #     result = count_peaks_from_waveform_optimized(wav_file,n=n, height=height, distance=distance)
                    #     ptk_result.loc[index_key, 'teo_3rd'] = result
                    elif 'p_9_' in filename:  # keo_3rd
                        result = count_peaks_from_waveform_wholetime(wav_file, n=n,height=height, distance=distance)
                        ptk_result.loc[index_key, 'keo_3rd'] = result
                    # elif 'p_12_' in filename:  # ptk_3rd
                    #     result = count_peaks_from_waveform_optimized(wav_file, n=n,height=height, distance=distance)
                    #     ptk_result.loc[index_key, 'ptk_3rd'] = result
                
                # ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
                if ptk_result.dropna(how='all').empty:
                    print("âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # ìƒê´€ê³„ìˆ˜ ê³„ì‚° - ê° ì—´ë³„ë¡œ
                ptk_result.index = ptk_result.index.astype(int)
                correlation_results = {}
                
                for col in ptk_result.columns:
                    if col in ptk_label.columns:
                        # ì¸ë±ìŠ¤ë¥¼ ë§ì¶˜ í›„ NaN ê°’ ì œê±°í•˜ì—¬ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
                        combined = pd.concat([ptk_result[col], ptk_label[col]], axis=1, join="inner").dropna()
                        
                        if not combined.empty and len(combined) > 1:
                            correlation = combined.corr(method='pearson').iloc[0, 1]
                            if not pd.isna(correlation):
                                correlation_results[col] = correlation
                            else:
                                correlation_results[col] = 0
                        else:
                            correlation_results[col] = 0
                
                if not correlation_results:
                    print("âŒ ìƒê´€ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨")
                    continue
                
                # í‰ê·  ìƒê´€ê³„ìˆ˜ ê³„ì‚°
                avg_correlation = np.mean(list(correlation_results.values()))
                max_correlation = max(correlation_results.values())
                
                elapsed_time = time.time() - start_time
                
                # ê²°ê³¼ ì €ì¥
                result_entry = {
                    'height': height,
                    'distance': distance,
                    'n':n,
                    'avg_correlation': avg_correlation,
                    'max_correlation': max_correlation,
                    'elapsed_time': elapsed_time,
                    'valid_samples': len(ptk_result.dropna(how='all'))
                }
                
                # ê° ì—´ë³„ ìƒê´€ê³„ìˆ˜ë„ ì €ì¥
                for col, corr in correlation_results.items():
                    result_entry[f'{col}_correlation'] = corr
                
                optimization_results.append(result_entry)
                
                print(f"âœ… í‰ê·  ìƒê´€ê³„ìˆ˜: {avg_correlation:.4f}, ìµœëŒ€: {max_correlation:.4f}, ì²˜ë¦¬ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
                
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
    
    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    results_df = pd.DataFrame(optimization_results)
    
    if results_df.empty:
        print("âŒ ìµœì í™” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, None
    
    # ê²°ê³¼ ì €ì¥
    results_df.to_csv('/Volumes/SSAM/aptk_model/ptk_3rd_optimization_results.csv', index=False)
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: /Volumes/SSAM/aptk_model/ptk_3rd_optimization_results.csv")
    
    # í‰ê·  ìƒê´€ê³„ìˆ˜ ê¸°ì¤€ ìµœê³  ê²°ê³¼
    best_avg = results_df.loc[results_df['avg_correlation'].idxmax()]
    
    # ìµœëŒ€ ìƒê´€ê³„ìˆ˜ ê¸°ì¤€ ìµœê³  ê²°ê³¼
    best_max = results_df.loc[results_df['max_correlation'].idxmax()]
    
    print("\nğŸ† ìµœì í™” ê²°ê³¼ (í‰ê·  ìƒê´€ê³„ìˆ˜ ê¸°ì¤€):")
    print("="*60)
    print(f"ìµœê³  í‰ê·  ìƒê´€ê³„ìˆ˜: {best_avg['avg_correlation']:.6f}")
    print(f"ìµœì  height: {best_avg['height']:.3f}")
    print(f"ìµœì  distance: {int(best_avg['distance'])}")
    print(f"ìµœì  n: {best_avg['n']}")
    print(f"ìœ íš¨ ìƒ˜í”Œ ìˆ˜: {int(best_avg['valid_samples'])}")
    print(f"ì²˜ë¦¬ ì‹œê°„: {best_avg['elapsed_time']:.2f}ì´ˆ")
    
    print("\nğŸ¯ ê°œë³„ ì—´ ìƒê´€ê³„ìˆ˜:")
    for col in ['peo_3rd', 'keo_3rd']:
        if f'{col}_correlation' in best_avg:
            print(f"  {col}: {best_avg[f'{col}_correlation']:.4f}")
    print("="*60)
    
    print("\nğŸ† ìµœì í™” ê²°ê³¼ (ìµœëŒ€ ìƒê´€ê³„ìˆ˜ ê¸°ì¤€):")
    print("="*60)
    print(f"ìµœê³  ìµœëŒ€ ìƒê´€ê³„ìˆ˜: {best_max['max_correlation']:.6f}")
    print(f"ìµœì  height: {best_max['height']:.3f}")
    print(f"ìµœì  distance: {int(best_max['distance'])}")
    print(f"ìµœì  n: {int(best_max['n'])}")

    print("="*60)
    
    return results_df, best_avg, best_max

def visualize_ptk_optimization_results(results_df):
    """
    PTK ìµœì í™” ê²°ê³¼ë¥¼ ì‹œê°í™”
    """
    print("ğŸ“Š ê²°ê³¼ ì‹œê°í™” ì¤‘...")
    
    fig, axes = plt.subplots(2, 3, figsize=(20, 12))
    
    # 1. í‰ê·  ìƒê´€ê³„ìˆ˜ íˆìŠ¤í† ê·¸ë¨
    axes[0, 0].hist(results_df['avg_correlation'], bins=30, alpha=0.7, color='lightblue', edgecolor='black')
    axes[0, 0].set_xlabel('Average Correlation Coefficient')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].set_title('Distribution of Average Correlation Coefficients')
    axes[0, 0].axvline(results_df['avg_correlation'].max(), color='red', linestyle='--', 
                       label=f'Max: {results_df["avg_correlation"].max():.4f}')
    axes[0, 0].legend()
    
    # 2. Height vs í‰ê·  ìƒê´€ê³„ìˆ˜
    scatter1 = axes[0, 1].scatter(results_df['height'], results_df['avg_correlation'], 
                                 c=results_df['distance'], cmap='viridis', alpha=0.6)
    axes[0, 1].set_xlabel('Height Threshold')
    axes[0, 1].set_ylabel('Average Correlation Coefficient')
    axes[0, 1].set_title('Height vs Average Correlation (colored by Distance)')
    plt.colorbar(scatter1, ax=axes[0, 1], label='Distance')
    
    # 3. Distance vs í‰ê·  ìƒê´€ê³„ìˆ˜
    scatter2 = axes[0, 2].scatter(results_df['distance'], results_df['avg_correlation'], 
                                 c=results_df['height'], cmap='plasma', alpha=0.6)
    axes[0, 2].set_xlabel('Distance Threshold')
    axes[0, 2].set_ylabel('Average Correlation Coefficient')
    axes[0, 2].set_title('Distance vs Average Correlation (colored by Height)')
    plt.colorbar(scatter2, ax=axes[0, 2], label='Height')
    
    # 4. íˆíŠ¸ë§µ - Height vs Distance
    pivot_table = results_df.pivot_table(values='avg_correlation', index='height', columns='distance', aggfunc='mean')
    im = axes[1, 0].imshow(pivot_table.values, aspect='auto', cmap='RdYlBu_r', origin='lower')
    axes[1, 0].set_xlabel('Distance Index')
    axes[1, 0].set_ylabel('Height Index')
    axes[1, 0].set_title('Average Correlation Heatmap')
    plt.colorbar(im, ax=axes[1, 0])
    
    # 5. Top 10 ê²°ê³¼
    top_10 = results_df.nlargest(10, 'avg_correlation')
    axes[1, 1].barh(range(len(top_10)), top_10['avg_correlation'])
    axes[1, 1].set_yticks(range(len(top_10)))
    axes[1, 1].set_yticklabels([f"h:{row['height']:.3f}, d:{int(row['distance'])}" 
                               for _, row in top_10.iterrows()])
    axes[1, 1].set_xlabel('Average Correlation Coefficient')
    axes[1, 1].set_title('Top 10 Parameter Combinations')
    
    # 6. ê°œë³„ ì—´ ìƒê´€ê³„ìˆ˜ ë¹„êµ (ìƒìœ„ 5ê°œ)
    top_5 = results_df.nlargest(5, 'avg_correlation')
    cols = ['peo_3rd_correlation', 'keo_3rd_correlation']
    
    for i, (_, row) in enumerate(top_5.iterrows()):
        values = [row.get(col, 0) for col in cols]
        axes[1, 2].bar([j + i*0.15 for j in range(len(cols))], values, 
                      width=0.15, label=f"h:{row['height']:.3f}, d:{int(row['distance'])}", alpha=0.7)
    
    axes[1, 2].set_xlabel('PTK Columns')
    axes[1, 2].set_ylabel('Correlation Coefficient')
    axes[1, 2].set_title('Individual Column Correlations (Top 5)')
    axes[1, 2].set_xticks(range(len(cols)))
    axes[1, 2].set_xticklabels(['peo_3rd', 'keo_3rd'])
    axes[1, 2].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    plt.tight_layout()
    plt.savefig('/Volumes/SSAM/aptk_model/ptk_3rd_optimization_visualization.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("ğŸ’¾ ì‹œê°í™” ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
    print("  - /Volumes/SSAM/aptk_model/ptk_3rd_optimization_visualization.png")

def analyze_ptk_parameter_sensitivity(results_df):
    """
    PTK íŒŒë¼ë¯¸í„°ì˜ ë¯¼ê°ë„ ë¶„ì„
    """
    print("ğŸ”¬ PTK íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë¶„ì„...")
    
    # ê° íŒŒë¼ë¯¸í„°ë³„ í‰ê·  ìƒê´€ê³„ìˆ˜
    height_sensitivity = results_df.groupby('height')['avg_correlation'].agg(['mean', 'std', 'count'])
    distance_sensitivity = results_df.groupby('distance')['avg_correlation'].agg(['mean', 'std', 'count'])
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“ˆ Height Threshold ë¯¼ê°ë„:")
    print(f"ìµœê³  í‰ê·  ìƒê´€ê³„ìˆ˜: {height_sensitivity['mean'].max():.4f} (height: {height_sensitivity['mean'].idxmax():.3f})")
    
    print("\nğŸ“ˆ Distance Threshold ë¯¼ê°ë„:")
    print(f"ìµœê³  í‰ê·  ìƒê´€ê³„ìˆ˜: {distance_sensitivity['mean'].max():.4f} (distance: {int(distance_sensitivity['mean'].idxmax())})")
    
    # ë¯¼ê°ë„ ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))
    
    # Height Threshold
    axes[0].errorbar(height_sensitivity.index, height_sensitivity['mean'], 
                     yerr=height_sensitivity['std'], capsize=3, marker='o')
    axes[0].set_xlabel('Height Threshold')
    axes[0].set_ylabel('Mean Average Correlation')
    axes[0].set_title('Height Threshold Sensitivity')
    axes[0].grid(True, alpha=0.3)
    
    # Distance Threshold
    axes[1].errorbar(distance_sensitivity.index, distance_sensitivity['mean'], 
                     yerr=distance_sensitivity['std'], capsize=3, marker='s')
    axes[1].set_xlabel('Distance Threshold')
    axes[1].set_ylabel('Mean Average Correlation')
    axes[1].set_title('Distance Threshold Sensitivity')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('/Volumes/SSAM/aptk_model/ptk_3rd_parameter_sensitivity_analysis.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    return height_sensitivity, distance_sensitivity

def create_excel_comparison(results_df, best_params):
    """
    ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ê³„ì‚°ëœ ê²°ê³¼ì™€ ì‹¤ì œ ë¼ë²¨ì„ Excelë¡œ ë¹„êµ
    """
    print("ğŸ“Š Excel ë¹„êµ íŒŒì¼ ìƒì„± ì¤‘...")
    
    # ë°ì´í„° ë¡œë“œ
    folder = '/Volumes/SSAM/project/files/upload'
    pattern = os.path.join(folder, '**', 'CLAP_D', '1', 'p*.wav')
    wav_files = glob.glob(pattern, recursive=True)
    
    df = pd.read_csv('/Volumes/SSAM/aptk_model/labeled_data_D.csv', header=1, index_col=0)
    ptk_label = df.loc[:, ['peo_3rd', 'keo_3rd']]
    
    # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ê²°ê³¼ ì¬ê³„ì‚°
    best_height = best_params['height']
    best_distance = best_params['distance']
    
    ptk_result = pd.DataFrame(columns=['peo_3rd', 'keo_3rd'])
    
    for wav_file in wav_files:
        parts = wav_file.replace(folder, '').strip(os.sep).split(os.sep)
        if len(parts) > 0:
            index_key = parts[0]
        else:
            index_key = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(wav_file))))
        
        if index_key not in ptk_result.index:
            ptk_result.loc[index_key] = [None, None]
        
        filename = os.path.basename(wav_file)
        
        if 'p_3_' in filename:
            result = count_peaks_from_waveform_wholetime(wav_file, n=best_params.get('n', 3), height=best_height, distance=best_distance)
            ptk_result.loc[index_key, 'peo_3rd'] = result
        # elif 'p_6_' in filename:
        #     result = count_peaks_from_waveform_optimized(wav_file, height=best_height, distance=best_distance)
        #     ptk_result.loc[index_key, 'teo_3rd'] = result
        elif 'p_9_' in filename:
            result = count_peaks_from_waveform_wholetime(wav_file, n=best_params.get('n', 3), height=best_height, distance=best_distance)
            ptk_result.loc[index_key, 'keo_3rd'] = result
        # elif 'p_12_' in filename:
        #     result = count_peaks_from_waveform_optimized(wav_file, n=best_params.get('n', 3), height=best_height, distance=best_distance)
        #     ptk_result.loc[index_key, 'ptk_3rd'] = result
    
    # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    comparison_data = []
    
    # ê³µí†µ í™˜ì ID ì°¾ê¸°
    ptk_result.index = ptk_result.index.astype(int)
    common_ids = set(df.index) & set(ptk_label.index) & set(ptk_result.index)
    
    for patient_id in sorted(common_ids):
        try:
            patient_number = df.loc[patient_id, 'number']
            
            # ì˜ˆì¸¡ ì´ì  ê³„ì‚° (ì‹¤ì œ ë¼ë²¨ì˜ í•©)
            predicted_total = ptk_label.loc[patient_id].sum()
            
            # ì‹¤ì œ ì´ì  ê³„ì‚° (ìµœì í™”ëœ ê²°ê³¼ì˜ í•©)
            actual_total = ptk_result.loc[patient_id].sum()
            
            # NaNì´ë‚˜ None ê°’ì´ ìˆëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸°
            if pd.isna(predicted_total) or pd.isna(actual_total):
                continue
            
            comparison_data.append({
                'í™˜ì ID': patient_number,
                'ì˜ˆì¸¡ ì´ì ': round(predicted_total, 1),
                'ì‹¤ì œ ì´ì ': round(actual_total, 1)
            })
            
        except (KeyError, TypeError, AttributeError) as e:
            # í™˜ì IDê°€ ì—†ê±°ë‚˜ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì‚­ì œ (ê±´ë„ˆë›°ê¸°)
            continue
    
    # DataFrame ìƒì„±
    comparison_df = pd.DataFrame(comparison_data)
    
    if comparison_df.empty:
        print("âŒ ë¹„êµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
    correlation = np.nan
    if len(comparison_df) > 1:
        try:
            from scipy.stats import pearsonr
            correlation, p_value = pearsonr(comparison_df['ì˜ˆì¸¡ ì´ì '], comparison_df['ì‹¤ì œ ì´ì '])
        except:
            correlation = best_params['avg_correlation']
    
    # ìƒê´€ê³„ìˆ˜ ì»¬ëŸ¼ ì¶”ê°€ (ì²« ë²ˆì§¸ í–‰ì—ë§Œ)
    comparison_df['ìƒê´€ê³„ìˆ˜'] = ''
    if not np.isnan(correlation):
        comparison_df.loc[0, 'ìƒê´€ê³„ìˆ˜'] = round(correlation, 3)
    
    # Excel íŒŒì¼ë¡œ ì €ì¥
    output_path = '/Volumes/SSAM/aptk_model/ptk_3rd_comparison.xlsx'
    
    try:
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            comparison_df.to_excel(writer, sheet_name='PTK_3rd_Comparison', index=False)
            
            # ì›Œí¬ì‹œíŠ¸ ê°€ì ¸ì˜¤ê¸°
            worksheet = writer.sheets['PTK_3rd_Comparison']
            
            # ì»¬ëŸ¼ ë„ˆë¹„ ì¡°ì •
            worksheet.column_dimensions['A'].width = 12
            worksheet.column_dimensions['B'].width = 12
            worksheet.column_dimensions['C'].width = 12
            worksheet.column_dimensions['D'].width = 12
        
        print(f"âœ… Excel ë¹„êµ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"ğŸ“ˆ ì „ì²´ í™˜ì ìˆ˜: {len(comparison_df)}")
        print(f"ğŸ”— ìƒê´€ê³„ìˆ˜: {correlation:.3f}" if not np.isnan(correlation) else "ğŸ”— ìƒê´€ê³„ìˆ˜: ê³„ì‚° ë¶ˆê°€")
        
    except Exception as e:
        print(f"âŒ Excel íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        # CSVë¡œ ëŒ€ì²´ ì €ì¥
        comparison_df.to_csv('/Volumes/SSAM/aptk_model/ptk_3rd_comparison.csv', index=False)
        print("ğŸ“„ CSV íŒŒì¼ë¡œ ëŒ€ì²´ ì €ì¥ ì™„ë£Œ: ptk_3rd_comparison.csv")

if __name__ == "__main__":
    # ìµœì í™” ì‹¤í–‰
    result = optimize_ptk_parameters()
    
    if result is None or result[0] is None:
        print("âŒ PTK ìµœì í™” ì‹¤íŒ¨: ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("   - WAV íŒŒì¼ ê²½ë¡œë‚˜ ë¼ë²¨ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        pass
    else:
        results_df, best_avg, best_max = result
        
        # ê²°ê³¼ ì‹œê°í™”
        visualize_ptk_optimization_results(results_df)
        
        # ë¯¼ê°ë„ ë¶„ì„
        height_sens, distance_sens = analyze_ptk_parameter_sensitivity(results_df)
        
        # ìµœì¢… ë³´ê³ ì„œ ì €ì¥
        with open('/Volumes/SSAM/aptk_model/ptk_3rd_optimization_report.txt', 'w', encoding='utf-8') as f:
            f.write("ğŸ† PTK íŒŒë¼ë¯¸í„° ìµœì í™” ë³´ê³ ì„œ\n")
            f.write("="*60 + "\n\n")
            
            f.write("ğŸ“Š í‰ê·  ìƒê´€ê³„ìˆ˜ ê¸°ì¤€ ìµœì  ê²°ê³¼:\n")
            f.write(f"ìµœê³  í‰ê·  ìƒê´€ê³„ìˆ˜: {best_avg['avg_correlation']:.6f}\n")
            f.write(f"ìµœì  height: {best_avg['height']:.3f}\n")
            f.write(f"ìµœì  distance: {int(best_avg['distance'])}\n")
            f.write(f"ìœ íš¨ ìƒ˜í”Œ ìˆ˜: {int(best_avg['valid_samples'])}\n")
            f.write(f"ì²˜ë¦¬ ì‹œê°„: {best_avg['elapsed_time']:.2f}ì´ˆ\n\n")
            
            f.write("ğŸ¯ ê°œë³„ ì—´ë³„ ìƒê´€ê³„ìˆ˜ (í‰ê·  ê¸°ì¤€):\n")
            for col in ['peo_3rd', 'keo_3rd']:
                if f'{col}_correlation' in best_avg:
                    f.write(f"  {col}: {best_avg[f'{col}_correlation']:.4f}\n")
            f.write("\n")
            
            f.write("ğŸ“Š ìµœëŒ€ ìƒê´€ê³„ìˆ˜ ê¸°ì¤€ ìµœì  ê²°ê³¼:\n")
            f.write(f"ìµœê³  ìµœëŒ€ ìƒê´€ê³„ìˆ˜: {best_max['max_correlation']:.6f}\n")
            f.write(f"ìµœì  height: {best_max['height']:.3f}\n")
            f.write(f"ìµœì  distance: {int(best_max['distance'])}\n\n")
            f.write(f"ìµœì  n: {int(best_max['n'])}\n\n")

            
            f.write("ğŸ“Š ìƒìœ„ 10ê°œ ê²°ê³¼ (í‰ê·  ìƒê´€ê³„ìˆ˜ ê¸°ì¤€):\n")
            f.write("-"*40 + "\n")
            top_10 = results_df.nlargest(10, 'avg_correlation')
            for idx, (_, row) in enumerate(top_10.iterrows(), 1):
                f.write(f"{idx:2d}. í‰ê·  ìƒê´€ê³„ìˆ˜: {row['avg_correlation']:.4f} | "
                       f"height: {row['height']:.3f} | "
                       f"distance: {int(row['distance'])}\n")
        
        print("\nğŸ’¾ ìµœì¢… ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: /Volumes/SSAM/aptk_model/ptk_3rd_optimization_report.txt")
        
        # Excel ë¹„êµ íŒŒì¼ ìƒì„±
        try:
            create_excel_comparison(results_df, best_avg)
        except Exception as e:
            print(f"âš ï¸ Excel íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        
        print("ğŸ‰ PTK ìµœì í™” íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")